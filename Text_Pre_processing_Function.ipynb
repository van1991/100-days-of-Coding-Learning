{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Pre-processing Function",
      "provenance": [],
      "collapsed_sections": [
        "3yYkXMN5bdVC",
        "3QB6fIROexOE",
        "fLru4lheh9aV",
        "-W8RriZxmhO8"
      ],
      "authorship_tag": "ABX9TyMQfaovzQnM4dHC9RjFju8k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/van1991/100-days-of-Coding-Learning/blob/master/Text_Pre_processing_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSELqq9DbYks",
        "colab_type": "text"
      },
      "source": [
        "## Feature Engineering on Text Data\n",
        "#### There can be multiple ways of cleaning and pre-processing textual data. In the following points, we highlight some of the most important ones which are used heavily in Natural Language Processing (NLP) pipelines.\n",
        "\n",
        "Reference: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yYkXMN5bdVC",
        "colab_type": "text"
      },
      "source": [
        "### **Removing tags:** Our text often contains unnecessary content like HTML tags, which do not add much value when analyzing text. The BeautifulSoup library does an excellent job in providing necessary functions for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OopgjemJbN2Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "9f1dce84-3a31-4c9b-91cf-100f7d1bef71"
      },
      "source": [
        "def remove_html_tags (text):\n",
        "  from bs4 import BeautifulSoup\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  html_stripped_text = soup.get_text()\n",
        "  print(\"1. html tags removed\")\n",
        "  return html_stripped_text\n",
        "\n",
        "remove_html_tags(document)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. html tags removed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Héllo! Héllo! can you hear me! I just heard about Python!\\r\\n \\n              It's an amazing language which can be used for Scripting, Web development,\\r\\n\\r\\n\\n              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\\n              What are you waiting for? Go and get started. He's learning, she's learning, they've already\\n\\n\\n              got a headstart!\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QB6fIROexOE",
        "colab_type": "text"
      },
      "source": [
        "### **Removing accented characters**: In any text corpus, especially if you are dealing with the English language, often you might be dealing with accented characters\\letters. Hence we need to make sure that these characters are converted and standardized into ASCII characters. A simple example would be converting é to e."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DtKy2DDewxf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "36301542-8e89-4c06-917d-bea9f7deece9"
      },
      "source": [
        "def remove_accented_chars(text):\n",
        "  import unicodedata\n",
        "  text = unicodedata.normalize ('NFKD', text).encode('ascii','ignore').decode('utf-8','ignore') # https://docs.python.org/3/howto/unicode.html\n",
        "  print(\"2. accented chars removed\")\n",
        "  return text\n",
        "\n",
        "remove_accented_chars(document)\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2. accented chars removed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<p>Hello! Hello! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \\n              It's an amazing language which can be used for Scripting, Web development,\\r\\n\\r\\n\\n              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\\n              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\\n              got a headstart!</p>\\n           \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLru4lheh9aV",
        "colab_type": "text"
      },
      "source": [
        "### **Expanding contractions**: In the English language, contractions are basically shortened versions of words or syllables. These shortened versions of existing words or phrases are created by removing specific letters and sounds. Examples would be, do not to don’t and I would to I’d. Converting each contraction to its expanded, original form often helps with text standardization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlT_yDeYiK0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from contractions import CONTRACTION_MAP # i think this contractions is a seperate python script on github\n",
        "\n",
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    print(\"3. expanding contractions\")\n",
        "    return expanded_text\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W8RriZxmhO8",
        "colab_type": "text"
      },
      "source": [
        "### **Removing special characters**: Special characters and symbols which are usually non alphanumeric characters often add to the extra noise in unstructured text. More than often, simple regular expressions (regexes) can be used to achieve this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZwiqfHJmlKj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "a0709d43-696d-4ef0-80d6-60998635de55"
      },
      "source": [
        "def remove_special_chars (text):\n",
        "  import re\n",
        "  text = re.sub('[^a-zA-z0-9\\s]','', text) # identify all non-alphanumeric characters and replace with '' i.e. remove them.\n",
        "  print(\"5. special charaters removed\")\n",
        "  return text\n",
        "\n",
        "remove_special_chars(document)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5. special charaters removed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'pHllo Hllo can you hear me I just heard about bPythonbbr\\r\\n \\n              Its an amazing language which can be used for Scripting Web development\\r\\n\\r\\n\\n              Information Retrieval Natural Language Processing Machine Learning  Artificial Intelligence\\n\\n              What are you waiting for Go and get startedbr Hes learning shes learning theyve already\\n\\n\\n              got a headstartp\\n           '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lw7AkRiqcnd",
        "colab_type": "text"
      },
      "source": [
        "### **Stemming and lemmatization**: Word stems are usually the base form of possible words that can be created by attaching affixes like prefixes and suffixes to the stem to create new words. This is known as inflection. The reverse process of obtaining the base form of a word is known as stemming. A simple example are the words WATCHES, WATCHING, and WATCHED. They have the word root stem WATCH as the base form. Lemmatization is very similar to stemming, where we remove word affixes to get to the base form of a word. However the base form in this case is known as the root word but not the root stem. The difference being that the root word is always a lexicographically correct word (present in the dictionary) but the root stem may not be so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA92QOvZqwjY",
        "colab_type": "text"
      },
      "source": [
        "#### There are multiple way to do stemming but it is not a good way to clean the data. Better approach is Lemmatization. But challenge with Lemmatization is that it is time consuming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW9gvssZqiBN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "b7b686a5-c5bc-47d8-e836-bfdbe8c5098a"
      },
      "source": [
        "def lemmatize(text):\n",
        "  import spacy\n",
        "  sp_nlp = spacy.load('en_core_web_sm')\n",
        "  #print(text)\n",
        "  text = sp_nlp(text) # creating spacy document. SpaCy automatically creates tokens.\n",
        "  #wo = [word.text for word in text]\n",
        "  #print(len(wo))\n",
        "  #print(wo)\n",
        "  lemma_text = \" \".join(word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text)\n",
        "  print(\"6. Lemmatization is done\")\n",
        "  return lemma_text\n",
        "\n",
        "lemmatize(document)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6. Lemmatization is done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'< p > héllo ! Héllo ! can you hear me ! I just hear about < b > python</b>!<br/ > \\r\\n \\n               It be an amazing language which can be use for Scripting , web development , \\r\\n\\r\\n\\n               Information Retrieval , Natural Language Processing , Machine Learning & Artificial Intelligence ! \\n\\n               what be you wait for ? go and get started.<br/ > He be learn , she be learn , they have already \\n\\n\\n               get a headstart!</p > \\n           '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Ea8X0GzWwK",
        "colab_type": "text"
      },
      "source": [
        "### **Removing stopwords**: Words which have little or no significance especially when constructing meaningful features from text are known as stopwords or stop words. These are usually words that end up having the maximum frequency if you do a simple term or word frequency in a corpus. Words like a, an, the, and so on are considered to be stopwords. There is no universal stopword list but we use a standard English language stopwords list from nltk. You can also add your own domain specific stopwords as needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHVkodCvzego",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "22284e4d-b70a-4e2b-e4d5-492764c84342"
      },
      "source": [
        "def remove_stopwords (text, is_lower_case = False):\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "  from nltk.tokenize.toktok import ToktokTokenizer\n",
        "  stopword_list = nltk.corpus.stopwords.words('english')\n",
        "  stopword_list.remove('no') # one can remove certain stopwords as per use-case\n",
        "  stopword_list.append('karthik') # one can add certain stopwords as per use-case\n",
        "  \n",
        "  tokenizer = ToktokTokenizer()\n",
        "  words = tokenizer.tokenize(text)\n",
        "  print('# of words before removing stopwords: ',len(words))\n",
        "  #if isinstance(words, list): print('it is a list') # to check if an object is a list\n",
        "  words = [w.strip() for w in words]  # removing additional whitespaces at the start or end of a word\n",
        "  if is_lower_case:\n",
        "    filtered_words = [w for w in words if w not in stopword_list]\n",
        "  else: \n",
        "      filtered_words = [w for w in words if w.lower() not in stopword_list]\n",
        "  filtered_text = ' '.join(filtered_words)\n",
        "  print('# of words after removing stopwords: ',len(filtered_words))\n",
        "  return filtered_text\n",
        "\n",
        "remove_stopwords(document)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "# of words before removing stopwords:  74\n",
            "# of words after removing stopwords:  49\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<p>Héllo ! Héllo ! hear ! heard <b>Python</b> ! <br/> ' amazing language used Scripting , Web development , Information Retrieval , Natural Language Processing , Machine Learning &amp; Artificial Intelligence ! waiting ? Go get started.<br/> ' learning , ' learning , ' already got headstart ! </p>\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_AH5xim8QB2",
        "colab_type": "text"
      },
      "source": [
        "### **Combine all the functions together to create single text pre-processing function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZUhiTcz8ZZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "1c49f2f6-ed49-4622-bf3c-d2c854120c61"
      },
      "source": [
        "def text_pre_processing (corpus,html_tags_removal = True, accented_chars_removal = True, \n",
        "                         expanding_contractions = False, text_lower_case = True,\n",
        "                         special_chars_removal = True, lemmatization = True,\n",
        "                         stopwords_removal = True ):\n",
        "  processed_corpus = []\n",
        "  # process each document in the corpus\n",
        "  for doc in corpus:\n",
        "    if html_tags_removal:\n",
        "      doc = remove_html_tags(doc)\n",
        "    if accented_chars_removal:\n",
        "      doc  = remove_accented_chars(doc)\n",
        "    if expanding_contractions:\n",
        "      doc = expand_contractions(doc)\n",
        "    # converting the text into lowercase\n",
        "    if text_lower_case:\n",
        "      doc = doc.lower()\n",
        "    # remove extra newline \\r,\\n,\\r\\n & whitespaces\n",
        "    import re\n",
        "    doc = re.sub('[\\r|\\n|\\r\\n| ]+', ' ',doc)\n",
        "    print(\"4. converted to lowercase and extra newlines & whitespaces removed\")\n",
        "    if special_chars_removal:\n",
        "      doc = remove_special_chars(doc)\n",
        "    if lemmatization:\n",
        "      doc = lemmatize(doc)\n",
        "    if stopwords_removal:\n",
        "      doc = remove_stopwords(doc)\n",
        "\n",
        "\n",
        "    processed_corpus.append(doc)\n",
        "  return processed_corpus\n",
        "\n",
        "doc2 = \"Karthik is a good boy\"\n",
        "doc3 = \"Peter's brother is also a good boy\"\n",
        "text_pre_processing([doc2, doc3, document])\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. html tags removed\n",
            "2. accented chars removed\n",
            "4. converted to lowercase and extra newlines & whitespaces removed\n",
            "5. special charaters removed\n",
            "6. Lemmatization is done\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "# of words before removing stopwords:  5\n",
            "# of words after removing stopwords:  2\n",
            "1. html tags removed\n",
            "2. accented chars removed\n",
            "4. converted to lowercase and extra newlines & whitespaces removed\n",
            "5. special charaters removed\n",
            "6. Lemmatization is done\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "# of words before removing stopwords:  7\n",
            "# of words after removing stopwords:  5\n",
            "1. html tags removed\n",
            "2. accented chars removed\n",
            "4. converted to lowercase and extra newlines & whitespaces removed\n",
            "5. special charaters removed\n",
            "6. Lemmatization is done\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "# of words before removing stopwords:  53\n",
            "# of words after removing stopwords:  29\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['good boy',\n",
              " 'peters brother also good boy',\n",
              " 'hello hello hear hear python amazing language use script web development information retrieval natural language processing machine learn artificial intelligence wait go get start learn learn already get headstart']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UMDsK8qNYsS",
        "colab_type": "text"
      },
      "source": [
        "# Can create word cloud out of the processed text. Try out tomorrow.\n",
        "Try using Counter from Collections for word frequeny counter\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQjCU3tCciKd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9a0a1349-3c7a-47d0-f77e-e0f118f40c9a"
      },
      "source": [
        "document = \"\"\"<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \n",
        "              It's an amazing language which can be used for Scripting, Web development,\\r\\n\\r\\n\n",
        "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\n",
        "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\n",
        "              got a headstart!</p>\n",
        "           \"\"\"\n",
        "document"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \\n              It's an amazing language which can be used for Scripting, Web development,\\r\\n\\r\\n\\n              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\\n              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\\n              got a headstart!</p>\\n           \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    }
  ]
}